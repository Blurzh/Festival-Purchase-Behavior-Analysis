{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d43e242",
   "metadata": {},
   "source": [
    "# Data Survey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd75f550",
   "metadata": {},
   "source": [
    "It's essential to understand the structure and quality of the dataset Iâ€™ll be working with. This first section focuses on exploring the data to identify potential issues, limitations, and opportunities for improvement.\n",
    "\n",
    "I plan to examine the following aspects, in order of priority:\n",
    "\n",
    "1. **Null Values**  \n",
    "   Are there any missing values? Which columns are affected, and how significant is the impact?\n",
    "\n",
    "2. **Inconsistent or Incorrect Entries (Typos)**  \n",
    "   Do any fields contain typos, extra spaces, or inconsistent formatting? Which columns need standarization?\n",
    "\n",
    "3. **Data Distributions**  \n",
    "   How do values distribute across key fields (e.g. satisfaction, ticket type, group size)? Do they appear natural or artificially uniform?\n",
    "\n",
    "4. **Data Consistency**  \n",
    "   Are related columns logically aligned?\n",
    "\n",
    "5. **Field Types and Optimization**  \n",
    "   Are the column data types appropriate? Can I optimize memory or performance through type conversion?\n",
    "\n",
    "6. **Outliers and Edge Cases**  \n",
    "   Are there any values that fall outside expected ranges (e.g. negative expenses, extreme ages)?\n",
    "\n",
    "This survey will shape the decisions I make in the [Data Sharpening]() and [Data Cleaning]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f03ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\PC\\Desktop\\Estudio\\Analisis de Datos\\Proyectos\\Festival Purchase Behavior Analysis\\Datasets\\festival_dataset_dirty_modified.csv\")\n",
    "\n",
    "# This part of the code provides an overview of the dataset, spaced, so each function is easier to read.\n",
    "\n",
    "print(df.shape)\n",
    "print(\"\\n\")\n",
    "print(df.head())\n",
    "print(\"\\n\")\n",
    "print(df.info())\n",
    "print(\"\\n\")\n",
    "print(df.describe())\n",
    "print(\"\\n\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\n\")\n",
    "print(df.duplicated().sum())\n",
    "print(\"\\n\")\n",
    "print(df.nunique())\n",
    "print(\"\\n\")\n",
    "print(df.columns)\n",
    "print(\"\\n\")\n",
    "\n",
    "# I'm selecting columns of type 'object' (text) or 'category' to analyze unique values in those columns.\n",
    "text_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Displaying unique values for each text column\n",
    "# With it, we can see the unique values in each text column, which helps us understand the dataset better\n",
    "# And detect typos or inconsistencies in the data.\n",
    "for col in text_columns:\n",
    "    print(f\"\\nUnique values of '{col}':\")\n",
    "    print(df[col].unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
